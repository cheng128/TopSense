{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subspecies', 'race']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "for synset in wn.synsets('subspecies'):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('race.n.01'),\n",
       " Synset('race.n.02'),\n",
       " Synset('race.n.03'),\n",
       " Synset('subspecies.n.01'),\n",
       " Synset('slipstream.n.01'),\n",
       " Synset('raceway.n.01'),\n",
       " Synset('rush.v.01'),\n",
       " Synset('race.v.02'),\n",
       " Synset('race.v.03'),\n",
       " Synset('race.v.04')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('race.n.03')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar = wn.synset('black_race.n.01')\n",
    "types_of_motorcar = motorcar.hypernyms()\n",
    "types_of_motorcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('black_race.n.01'),\n",
       " Synset('color.n.04'),\n",
       " Synset('indian_race.n.01'),\n",
       " Synset('indian_race.n.02'),\n",
       " Synset('master_race.n.01'),\n",
       " Synset('slavic_people.n.01'),\n",
       " Synset('white_race.n.01'),\n",
       " Synset('yellow_race.n.01')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar = wn.synset('race.n.03')\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08333333333333333"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right = wn.synset('ethnology.n.01') \n",
    "minke = wn.synset('race.n.03')\n",
    "# 「露脊鯨」與「小鬚鯨」在上位詞組中最低位的詞組\n",
    "right.lowest_common_hypernyms(minke)\n",
    "right.path_similarity(minke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-roberta-large-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flavour \t\t Sensation \t\t Score: 0.4958\n"
     ]
    }
   ],
   "source": [
    "taste = ['FLAVOUR'.lower()]\n",
    "category = ['Sensation']\n",
    "\n",
    "embeddings1 = model.encode(taste, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(category, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(taste)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(taste[i], category[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wiki/wiki_word_id2sents.json') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/remap.word_id.topics.examples.json') as f:\n",
    "    word_id2topics = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in data:\n",
    "    if word.startswith('slug.noun'):\n",
    "        if word in word_id2topics:\n",
    "            print(word)\n",
    "            print(word_id2topics[word]['topics'])\n",
    "            print(data[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in data:\n",
    "    if word.startswith('bank.noun'):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wiki/wiki_href2def.json') as f:\n",
    "    wiki = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in wiki:\n",
    "    if '(geology)' in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'The banks of Switzerland and the insurance companies in Switzerland together produce eleven per cent of the gross domestic product.'\n",
    "for word in wiki:\n",
    "    if 'bank' in wiki[word][0]:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/jsonl_file/0.0_top3_brt_map_cam.jsonl') as f:\n",
    "    brt = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/words2defs.json') as f:\n",
    "    word2def = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentences = word2def['taste'] + category['Sensation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = defaultdict(list)\n",
    "for data in brt:\n",
    "    if data['brt_word'] != 'taste':\n",
    "        category[data['category']].append(data['en_def'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "\n",
    "model = SentenceTransformer('all-roberta-large-v1')\n",
    "corpus_embeddings = model.encode(corpus_sentences, batch_size=64, show_progress_bar=True, convert_to_tensor=True)\n",
    "print(\"Start clustering\")\n",
    "start_time = time.time()\n",
    "\n",
    "clusters = util.community_detection(corpus_embeddings, min_community_size=1, threshold=0.2)\n",
    "\n",
    "print(\"Clustering done after {:.2f} sec\".format(time.time() - start_time))\n",
    "\n",
    "#Print for all clusters the top 3 and bottom 3 elements\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(\"\\nCluster {}, #{} Elements \".format(i+1, len(cluster)))\n",
    "    for sentence_id in cluster[0:3]:\n",
    "        print(\"\\t\", corpus_sentences[sentence_id])\n",
    "    print(\"\\t\", \"...\")\n",
    "    for sentence_id in cluster[-3:]:\n",
    "        print(\"\\t\", corpus_sentences[sentence_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/merged_brt_group.json') as f:\n",
    "    merged_brt = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = {}\n",
    "for k, v in merged_brt.items():\n",
    "    for idx, group in enumerate(v):\n",
    "        for g in group:\n",
    "            name[g] = k + '_' + str(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/original_new.json', 'w') as f:\n",
    "#     f.write(json.dumps(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/word_id.topics.examples.json') as f:\n",
    "    original = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "for key, value in original.items():\n",
    "    temp = []\n",
    "    for group in value['topics']:\n",
    "        topic = group.split(' ', 1)[1]\n",
    "        temp.append(name[topic])\n",
    "    new_dict[key] = {'topics':list(set(temp)),\n",
    "                    'examples': value['examples'],\n",
    "                    'pos': value['pos'],\n",
    "                    'headword': value['headword']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/merged_word_id.topics.json', 'w') as f:\n",
    "    f.write(json.dumps(new_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Graduate",
   "language": "python",
   "name": "graduate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
